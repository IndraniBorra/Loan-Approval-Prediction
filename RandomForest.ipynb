{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=UndefinedMetricWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (assuming already in df)\n",
    "df=pd.read_csv(\"df_model.csv\")\n",
    "# Define the features and target variable\n",
    "    \n",
    "# Shuffle and split the data into features (X) and target (y)\n",
    "X = df.drop(columns=[\"loan_status\"])  # Feature columns\n",
    "y = df[\"loan_status\"]  # Target column\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Decision Tree from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeScratch:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def _gini(self, groups, classes):\n",
    "        # Calculate Gini index for split groups\n",
    "        gini = 0.0\n",
    "        total_samples = sum([len(group) for group in groups])\n",
    "        for group in groups:\n",
    "            size = len(group)\n",
    "            if size == 0:\n",
    "                continue\n",
    "            score = 0.0\n",
    "            for class_val in classes:\n",
    "                proportion = [row[-1] for row in group].count(class_val) / size\n",
    "                score += proportion**2\n",
    "            gini += (1 - score) * (size / total_samples)\n",
    "        return gini\n",
    "\n",
    "    def _split(self, index, value, dataset):\n",
    "        left, right = [], []\n",
    "        for row in dataset:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "\n",
    "    def _best_split(self, dataset):\n",
    "        class_values = list(set(row[-1] for row in dataset))\n",
    "        best_index, best_value, best_score, best_groups = None, None, float(\"inf\"), None\n",
    "        for index in range(len(dataset[0]) - 1):\n",
    "            for row in dataset:\n",
    "                groups = self._split(index, row[index], dataset)\n",
    "                gini = self._gini(groups, class_values)\n",
    "                if gini < best_score:\n",
    "                    best_index, best_value, best_score, best_groups = index, row[index], gini, groups\n",
    "        return {\"index\": best_index, \"value\": best_value, \"groups\": best_groups}\n",
    "\n",
    "    def _terminal(self, group):\n",
    "        outcomes = [row[-1] for row in group]\n",
    "        return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "    def _split_node(self, node, depth):\n",
    "        left, right = node[\"groups\"]\n",
    "        del node[\"groups\"]\n",
    "        if not left or not right:\n",
    "            node[\"left\"] = node[\"right\"] = self._terminal(left + right)\n",
    "            return\n",
    "        if depth >= self.max_depth:\n",
    "            node[\"left\"], node[\"right\"] = self._terminal(left), self._terminal(right)\n",
    "            return\n",
    "        node[\"left\"] = self._best_split(left)\n",
    "        self._split_node(node[\"left\"], depth + 1)\n",
    "        node[\"right\"] = self._best_split(right)\n",
    "        self._split_node(node[\"right\"], depth + 1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dataset = np.hstack((X, y.reshape(-1, 1)))\n",
    "        self.tree = self._best_split(dataset)\n",
    "        self._split_node(self.tree, 1)\n",
    "\n",
    "    def _predict(self, node, row):\n",
    "        if row[node[\"index\"]] < node[\"value\"]:\n",
    "            if isinstance(node[\"left\"], dict):\n",
    "                return self._predict(node[\"left\"], row)\n",
    "            else:\n",
    "                return node[\"left\"]\n",
    "        else:\n",
    "            if isinstance(node[\"right\"], dict):\n",
    "                return self._predict(node[\"right\"], row)\n",
    "            else:\n",
    "                return node[\"right\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(self.tree, row) for row in X]\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Random Forest from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestScratch:\n",
    "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, sample_size=1.0):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.sample_size = sample_size\n",
    "        self.trees = []\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = round(len(X) * self.sample_size)\n",
    "        indices = np.random.choice(len(X), n_samples, replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            tree = DecisionTreeScratch(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.round(np.mean(predictions, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy for the scratch implementation\n",
    "X_train_np, X_test_np = X_train.values, X_test.values\n",
    "y_train_np, y_test_np = y_train.values, y_test.values\n",
    "\n",
    "# Train Random Forest from scratch\n",
    "rf_scratch = RandomForestScratch(n_trees=10, max_depth=1)\n",
    "rf_scratch.fit(X_train_np, y_train_np)\n",
    "\n",
    "# Train scikit-learn Random Forest\n",
    "rf_sklearn = RandomForestClassifier(n_estimators=10, max_depth=1, random_state=42)\n",
    "rf_sklearn.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_scratch = rf_scratch.predict(X_test_np)\n",
    "y_pred_sklearn = rf_sklearn.predict(X_test)\n",
    "\n",
    "# Accuracy Comparison\n",
    "accuracy_scratch = accuracy_score(y_test_np, y_pred_scratch)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"Scratch Random Forest Accuracy: {accuracy_scratch}\")\n",
    "print(f\"Scikit-learn Random Forest Accuracy: {accuracy_sklearn}\")\n",
    "\n",
    "# Metrics Visualization\n",
    "conf_matrix_scratch = confusion_matrix(y_test_np, y_pred_scratch)\n",
    "conf_matrix_sklearn = confusion_matrix(y_test, y_pred_sklearn)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.heatmap(conf_matrix_scratch, annot=True, fmt=\"d\", ax=axes[0], cmap=\"Blues\")\n",
    "axes[0].set_title(\"Scratch RF Confusion Matrix\")\n",
    "sns.heatmap(conf_matrix_sklearn, annot=True, fmt=\"d\", ax=axes[1], cmap=\"Blues\")\n",
    "axes[1].set_title(\"Sklearn RF Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Scratch): 0.7765921973991331\n",
      "Confusion Matrix (Scratch):\n",
      " [[6987    0]\n",
      " [2010    0]]\n",
      "Classification Report (Scratch):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.87      6987\n",
      "           1       0.00      0.00      0.00      2010\n",
      "\n",
      "    accuracy                           0.78      8997\n",
      "   macro avg       0.39      0.50      0.44      8997\n",
      "weighted avg       0.60      0.78      0.68      8997\n",
      "\n",
      "\n",
      "Accuracy (sklearn): 0.7765921973991331\n",
      "Confusion Matrix (sklearn):\n",
      " [[6987    0]\n",
      " [2010    0]]\n",
      "Classification Report (sklearn):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.87      6987\n",
      "           1       0.00      0.00      0.00      2010\n",
      "\n",
      "    accuracy                           0.78      8997\n",
      "   macro avg       0.39      0.50      0.44      8997\n",
      "weighted avg       0.60      0.78      0.68      8997\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indraniborra/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/indraniborra/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/indraniborra/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/indraniborra/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/indraniborra/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/indraniborra/.pyenv/versions/3.9.6/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"df_model.csv\")\n",
    "\n",
    "# Define the features and target variable\n",
    "X = df.drop(columns=[\"loan_status\"]).values  # Feature columns\n",
    "y = df[\"loan_status\"].values  # Target column\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Optimized Random Forest from Scratch\n",
    "class DecisionTreeScratch:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def _gini(self, groups, classes):\n",
    "        gini = 0.0\n",
    "        total_samples = sum(len(group) for group in groups)\n",
    "        for group in groups:\n",
    "            size = len(group)\n",
    "            if size == 0:\n",
    "                continue\n",
    "            score = 0.0\n",
    "            for c in classes:\n",
    "                proportion = np.sum(group[:, -1] == c) / size  # Fix for class counting\n",
    "                score += proportion ** 2\n",
    "            gini += (1 - score) * (size / total_samples)\n",
    "        return gini\n",
    "\n",
    "    def _split(self, index, value, dataset):\n",
    "        left = dataset[dataset[:, index] < value]\n",
    "        right = dataset[dataset[:, index] >= value]\n",
    "        return left, right\n",
    "\n",
    "    def _best_split(self, dataset):\n",
    "        class_values = np.unique(dataset[:, -1])  # Fix to get unique classes\n",
    "        best_index, best_value, best_score, best_groups = None, None, float(\"inf\"), None\n",
    "        for index in range(dataset.shape[1] - 1):\n",
    "            for row in dataset:\n",
    "                groups = self._split(index, row[index], dataset)\n",
    "                gini = self._gini(groups, class_values)\n",
    "                if gini < best_score:\n",
    "                    best_index, best_value, best_score, best_groups = index, row[index], gini, groups\n",
    "        return {\"index\": best_index, \"value\": best_value, \"groups\": best_groups}\n",
    "\n",
    "    def _terminal(self, group):\n",
    "        outcomes = group[:, -1]\n",
    "        return np.bincount(outcomes.astype(int)).argmax()  # Fix for terminal value\n",
    "\n",
    "    def _split_node(self, node, depth):\n",
    "        left, right = node[\"groups\"]\n",
    "        del node[\"groups\"]\n",
    "        if not len(left) or not len(right):\n",
    "            node[\"left\"] = node[\"right\"] = self._terminal(np.vstack((left, right)))\n",
    "            return\n",
    "        if depth >= self.max_depth:\n",
    "            node[\"left\"], node[\"right\"] = self._terminal(left), self._terminal(right)\n",
    "            return\n",
    "        node[\"left\"] = self._best_split(left)\n",
    "        self._split_node(node[\"left\"], depth + 1)\n",
    "        node[\"right\"] = self._best_split(right)\n",
    "        self._split_node(node[\"right\"], depth + 1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dataset = np.hstack((X, y.reshape(-1, 1)))\n",
    "        self.tree = self._best_split(dataset)\n",
    "        self._split_node(self.tree, 1)\n",
    "\n",
    "    def _predict(self, node, row):\n",
    "        if row[node[\"index\"]] < node[\"value\"]:\n",
    "            if isinstance(node[\"left\"], dict):\n",
    "                return self._predict(node[\"left\"], row)\n",
    "            else:\n",
    "                return node[\"left\"]\n",
    "        else:\n",
    "            if isinstance(node[\"right\"], dict):\n",
    "                return self._predict(node[\"right\"], row)\n",
    "            else:\n",
    "                return node[\"right\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(self.tree, row) for row in X])\n",
    "\n",
    "\n",
    "# Train Random Forest from scratch\n",
    "rf_scratch = RandomForestScratch(n_trees=5, max_depth=1, min_samples_split=10)\n",
    "rf_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Predict with Random Forest from scratch\n",
    "y_pred_scratch = rf_scratch.predict(X_test)\n",
    "\n",
    "# Train scikit-learn Random Forest\n",
    "rf_sklearn = RandomForestClassifier(n_estimators=5, max_depth=1, random_state=42)\n",
    "rf_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = rf_sklearn.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy (Scratch):\", accuracy_score(y_test, y_pred_scratch))\n",
    "print(\"Confusion Matrix (Scratch):\\n\", confusion_matrix(y_test, y_pred_scratch))\n",
    "print(\"Classification Report (Scratch):\\n\", classification_report(y_test, y_pred_scratch))\n",
    "\n",
    "print(\"\\nAccuracy (sklearn):\", accuracy_score(y_test, y_pred_sklearn))\n",
    "print(\"Confusion Matrix (sklearn):\\n\", confusion_matrix(y_test, y_pred_sklearn))\n",
    "print(\"Classification Report (sklearn):\\n\", classification_report(y_test, y_pred_sklearn))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
